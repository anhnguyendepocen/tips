---
title: "R Matrix Algebra and Linear Algebra"
author: "J. R. Minter (editor)"
date: "Started: 2018-06-11, Last modified: 2018-10-11"
output:
  html_document:
    css: ../theme/jm-gray-vignette.css
    number_sections: yes
    toc: yes
    toc_depth: 3
---

[Back to Index](../README.html)

Load the required packages at the top...

```{r loadLibraries, message=FALSE, comment=NA}
library(rafalib)
library(ggplot2)
library(UsingR)
```

# Introduction

R has operators and functions for efficient matrix algebra. This
document was adapted from two sources:

1. YouTube videos by Ed Boone of Virginia Commonwealth University:
[Part 1](https://www.youtube.com/watch?v=0uXGlikIPX0),
[Part 2](https://www.youtube.com/watch?v=igrxg5wvA-0&t=144s),
and    
[Part 3](https://www.youtube.com/watch?v=V4L79XOxMpY). The
entire series is quite helpful.

2. A document from Rafael A Irizarry of Harvard
([(rafalab)](https://github.com/genomicsclass/labs/tree/master/matrixalg)
on Github.)

## Greek letters

We would prefer to avoid Greek letters, but they are common in the
statistical literature so we want you to become used to them.
They are mainly used to distinguish the unknown from the observed.
Suppose we want to find out the average height of a population and we
take a sample of 1,000 people to estimate this. The unknown average we
want to estimate is often denoted with $\mu$, the Greek letter for m
(**m is for mean**). The **standard deviation** is often denoted with
$\sigma$, the Greek letter for s. Measurement error or other unexplained
random variability is typically denoted with $\varepsilon$, the Greek
letter for e. Effect sizes, for example the effect of a diet on weight,
are typically denoted with $\beta$. We may use other Greek letters but
those are the most commonly used. 

You should get used to these four Greek letters as you will be seeing
them often: $\mu$, $\sigma$, $\beta$ and $\varepsilon$.

## Matrix Notation

Here we introduce the basics of matrix notation. Initially this may seem
over-complicated, but once we discuss examples, you will appreciate the
power of using this notation to both explain and derive solutions, as
well as implement them as R code.

## The language of linear models

Linear algebra notation actually simplifies the mathematical
descriptions and manipulations of linear models, as well as coding in R.
We will discuss the basics of this notation and then show some examples
in R.

The main point of this entire exercise is to show how we can write the
models above using matrix notation, and then explain how this is useful
for solving the least squares equation. We start by simply defining
notation and matrix multiplication, but bear with us since we eventually
get back to the practical application.

## Solving Systems of Equations

Linear algebra was created by mathematicians to solve systems of linear
equations such as this:

$$
\begin{align*}
a + b + c &= 6\\
3a - 2b + c &= 2\\
2a + b  - c &= 1
\end{align*}
$$

It provides very useful machinery to solve these problems generally.
We will learn how we can write and solve this system using matrix
algebra notation:

$$ 
\,
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix} =
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
\implies
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix} =
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}^{-1}
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
$$
# Scalars

Finally, we define a scalar. A scalar is just a number, which we call a
scalar because we want to distinguish it from vectors and matrices. We
**usually use lower case and don't bold**. In the next section, we will
understand why we make this distinction.


# Vectors

R lets us define vectors

```{r DefineTwoVectors}
a1 <- c(1,3,4,2,5)
b1 <- c(2,1,3,5,4)
```

## Simple addition of vectors

We can add a **constant to each element.**


```{r, comment=NA}
a5 <- a1 + 5
print(a1)
print(a5)
```

Note that if we add `a1` and `b1` together, they add
**element by element**!


```{r, comment=NA}
ab <- a1 + b1
print(ab)
```

## Simple multiplication of vectors

```{r, comment=NA}
print(a1)
print(5*a1)  # multiplies by a scalar
a_times_b <- a1*b1
print(a_times_b) # element by element multiplication
```

Note how we select (and print) a **single element** of a vector.

```{r, comment=NA}
print(a_times_b[2]) # print the second element
```

# Matrices

Note that **by default**, R fills the matrix **column-by-column**.

```{r, comment=NA}
m1 <- matrix(seq(1:4), nrow=2, ncol=2)
m2 <- matrix(c(4,3,2,1), nrow=2, ncol=2)
print(m1)
```

We can also write it like this:

```{r, comment=NA}
N <- 100; p <- 5
X <- matrix(1:(N*p),N,p)
head(X)
dim(X)
```

When it is convenient for a particular data source, we can
**override the default** and force R to generate the matrix
**row by row** using the `byrow=TRUE` argument. An example is
shown below:

```{r createMatrixRowByRow, comment=NA}
N <- 100; p <- 5
X <- matrix(1:(N*p),N,p,byrow=TRUE)
head(X)
dim(X)
```



## True matrix multiplication

For true matrix multiplication, you need an `m x n` matrix times an
`n x p` matrix.

```{r, comment=NA}
n1 <- matrix(seq(1:8), nrow=2, ncol=4)
n2 <- matrix(seq(1:24), nrow=4, ncol=6)
n1
n2
n1 %*% n2
```

# Using Linear Algebra

Now we are ready to see how matrix algebra can be useful when analyzing
data. We start with some simple examples and eventually arrive at the
main one: how to write linear models with matrix algebra notation and
solve the least squares problem.

## The average

To compute the sample average and variance of our data, we use these
formulas $\bar{Y}=\frac{1}{N} Y_i$ and
$\mbox{var}(Y)=\frac{1}{N} \sum_{i=1}^N (Y_i - \bar{Y})^2$. We can
represent these with matrix multiplication. First, define this
$N \times 1$ matrix made just of `1s`:

$$
A=\begin{pmatrix}
1\\
1\\
\vdots\\
1
\end{pmatrix}
$$

This implies that:

$$
\frac{1}{N}
\mathbf{A}^\top Y = \frac{1}{N}
\begin{pmatrix}1&1&\dots&1\end{pmatrix}
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}=
\frac{1}{N} \sum_{i=1}^N Y_i
= \bar{Y}
$$

Note that we are multiplying by the scalar $1/N$. In R, we multiply
matrix using `%*%`:


```{r,message=FALSE, comment=NA}
data(father.son, package="UsingR")
y <- father.son$sheight
print(mean(y))
```
We can also do this explicitly using linear algebra: 

```{r, message=FALSE, comment=NA}
N <- length(y)
Y<- matrix(y,N,1)
A <- matrix(1,N,1)
barY=t(A)%*%Y / N

print(barY)
```

## The variance

As we will see later, multiplying the transpose of a matrix with another
is very common in statistics. In fact, it is so common that there is a
function in R:

```{r, message=FALSE, comment=NA}
barY=crossprod(A,Y) / N
print(barY)
```

For the variance, we note that if:

$$
\mathbf{r}\equiv \begin{pmatrix}
Y_1 - \bar{Y}\\
\vdots\\
Y_N - \bar{Y}
\end{pmatrix}, \,\,
\frac{1}{N} \mathbf{r}^\top\mathbf{r} = 
\frac{1}{N}\sum_{i=1}^N (Y_i - \bar{Y})^2
$$

In R, if you only send one matrix into `crossprod`, it computes
$r^\top r$ so we can simply type:

```{r, message=FALSE, comment=NA}
r <- y - barY
crossprod(r)/N
```

Which is almost equivalent to `popvar` (from the `rafalab` package):
```{r, message=FALSE, comment=NA}
popvar(y) 
```

## Linear models

Now we are ready to put all this to use. Let's start with Galton's
example. If we define these matrices:
 
$$
\mathbf{Y} = \begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}
,
\mathbf{X} = \begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_N
\end{pmatrix}
,
\mathbf{\beta} = \begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix} \mbox{ and }
\mathbf{\varepsilon} = \begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}
$$

Then we can write the model:

$$ 
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, i=1,\dots,N 
$$

as: 


$$
\,
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix} = 
\begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_N
\end{pmatrix}
\begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix} +
\begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}
$$

or simply: 

$$
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}
$$

which is a much simpler way to write it. 


The least squares equation becomes simpler as well since it is the
following cross-product:

$$
(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})^\top
(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})
$$

So now we are ready to determine which values of $\beta$ minimize the
above, which we can do using calculus to find the minimum.

## Advanced: Finding the minimum using calculus

There are a series of rules that permit us to compute partial derivative
equations in matrix notation. By equating the derivative to 0 and
solving for the $\beta$, we will have our solution. The only one we need
here tells us that the derivative of the above equation is:

$$
2 \mathbf{X}^\top (\mathbf{Y} - \mathbf{X} \boldsymbol{\hat{\beta}})=0
$$

$$
\mathbf{X}^\top \mathbf{X} \boldsymbol{\hat{\beta}} = \mathbf{X}^\top \mathbf{Y}   
$$


$$
\boldsymbol{\hat{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}   
$$

and we have our solution. We usually put a hat on the $\beta$ that
solves this, $\hat{\beta}$ , as it is an estimate of the "real" $\beta$
that generated the data.

Remember that the least squares are like a square (multiply something
by itself) and that this formula is similar to the derivative of
$f(x)^2$ being $2f(x)f\prime (x)$. 

## Finding LSE in R

Let's see how it works in R:

```{r, message=FALSE, comment=NA}
data(father.son,package="UsingR")
x=father.son$fheight
y=father.son$sheight
X <- cbind(1,x)
betahat <- solve( t(X) %*% X ) %*% t(X) %*% y
```

or...

```{r, message=FALSE, comment=NA}
betahat <- solve( crossprod(X) ) %*% crossprod( X, y )
betahat
```





Now we can see the results of this by computing the estimated $\hat{\beta}_0+\hat{\beta}_1 x$ for any value of $x$:

```{r galton_regression_line, fig.width=9.25, fig.cap="Standard graphic plot of Galton's data with fitted regression line."}
newx <- seq(min(x),max(x),len=100)
X <- cbind(1,newx)
fitted <- X%*%betahat
plot(x,y,xlab="Father's height",ylab="Son's height")
lines(newx,fitted,col=2)
```

Try with ggplot...

```{r, message=FALSE, comment=NA, fig.width=9.25, fig.cap="ggplot2 (new graphics) plot of Galton's data with fitted regression line."}
df <- data.frame(x=x, y=y)
plt <-	ggplot(data=df) +
		geom_point( aes(x=x, y=y), colour="darkblue") +
		geom_smooth(aes(x=x, y=y), colour="darkred", size=2,
					method = "lm", se = FALSE) +
		xlab("Father's Height") +
		ylab("Son's Height") +
		ggtitle("Galton's Father/Son Heights") +
		theme(axis.text=element_text(size=12),
			  axis.title=element_text(size=12),
			  plot.title=element_text(hjust = 0.5)) # center the title

print(plt)

```



This $\hat{\boldsymbol{\beta}}=(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}$
is one of the most widely used results in data analysis. One of the
advantages of this approach is that we can use it in many different situations.
For example, in our falling object problem: 
 
```{r}
set.seed(1)
g <- 9.8 #meters per second
n <- 25
tt <- seq(0,3.4,len=n) #time in secs, t is a base function
d <- 56.67  - 0.5*g*tt^2 + rnorm(n,sd=1)
```

Notice that we are using almost the same exact code:


```{r gravity_with_fitted_parabola, comment=NA, fig.width=9.25, fig.cap="Fitted parabola to simulated data for distance traveled versus time of falling object measured with error."}
X <- cbind(1,tt,tt^2)
y <- d
betahat <- solve(crossprod(X))%*%crossprod(X,y)
newtt <- seq(min(tt),max(tt),len=100)
X <- cbind(1,newtt,newtt^2)
fitted <- X%*%betahat
plot(tt,y,xlab="Time",ylab="Height")
lines(newtt,fitted,col=2)
```

and with ggplot2...

```{r ggplot_gravity_with_fitted_parabola, comment=NA, fig.width=9.25, fig.cap="Fitted parabola to simulated data for distance traveled versus time of falling object measured with error."}
df <- data.frame(x=tt, y=y)
df_fit <-data.frame(x=newtt, y=fitted)

pltGrav <-	ggplot(data=df) +
			geom_point( aes(x=x, y=y), colour="darkblue", size=3) +
			geom_line(data=df_fit, aes(x=x, y=y),
					  color='darkred', size=1.5) +
			xlab("Time [s]") + ylab("Height [m]") +
			ggtitle("Galileo's Experiment") +
			theme(axis.text=element_text(size=12),
			 	  axis.title=element_text(size=12),
			 	  plot.title=element_text(hjust = 0.5)) # center title
			

print(pltGrav)
```

And the resulting estimates are what we expect:

```{r, message=FALSE, comment=NA}
betahat
```

The Tower of Pisa is about 56 meters high. Since we are just dropping the object there is no initial velocity, and half the constant of gravity is 9.8/2=4.9 meters per second squared.

#### The `lm` Function
R has a very convenient function that fits these models. We will learn more about this function later, but here is a preview:

```{r, message=FALSE, comment=NA}
X <- cbind(tt,tt^2)
fit=lm(y~X)
summary(fit)
```

Note that we obtain the same values as above.

# Summary

We have shown how to write linear models using linear algebra. We are going to do this for several examples, many of which are related to designed experiments. We also demonstrated how to obtain least squares estimates. Nevertheless, it is important to remember that because $y$ is a random variable, these estimates are random as well. In a later section, we will learn how to compute standard error for these estimates and use this to perform inference.






[Back to Index](../README.html)