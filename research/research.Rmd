---
title: "Research Tips"
author: "J. R. Minter"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  knitr:::html_vignette:
    css: ../theme/jm-mod-vignette.css
    number_sections: yes
    toc: yes
vignette: >
  %\VignetteIndexEntry{Research Tips}
  %\VignetteEngine{knitr::rmarkdown}
---

[Back to Index](../README.html)

These are tips on effective scientific research collected by John Minter from selected web sources.

# Software Carpentry

Greg Wilson is an excellent teacher!

Some files that we need are available [here](https://files.software-carpentry.org/). There is also a version of the course taught by Tiziano Zito at the [Berlin Institute of Technology](https://itb.biologie.hu-berlin.de/~zito/teaching/SC/) that has all the files we need (download the zipped archive and look in the `lec/inc/` directory) for all the files needed for the course from 2012 that was recorded by Greg for the [youtube](https://www.youtube.com/playlist?list=PLhFTuW7KWApwoo2DHzpWLhA19zenWF1LF) playlist.



## Key suggestions:

1. Greg begins by noting that our work is really **'brain limited'**. Neuroscience research studies show that people can hold **7 $\pm$ 2** unrelated facts in short term memory. We then develop **'mental inter-relations'** between these to form patterns. **Example:** We do not recall the five dots on a dice as separate facts but as a pattern or unit. We call this **chunking**. Forming these interconnections is part of long term memory.  The **key take-away** is that we **work best** when we **accommodate our physiology**. For example, Write and test logical units that fit the 7 $\pm$ 2 items one can keep in short term memory and assemble the into functions/blocks. **Note:** The shell history or iPython history can save typing and help. 

2. Plan your work in approximately **45 to 60 minute segments** where you have optimal focus. It is important to get up, walk around and get some oxygen to be refreshed. **Example:** Cadets at the U. S. Military Academies are taught to do push-ups to refresh their focus.

3. Test and debug as you go. This saves time in the long run.

4. Commit short, logical blocks to version control using meaningful commit messages to aid in later search.

5. **The importance of peer instruction:** From Eric Mazur at Harvard. If we get a misconception, likely someone in the group will understand and explain the misconception. Recent learners are often much better at this than experts. I call this **the importance of a second pair of eyes**.

6. **Clarity is more important that brevity**. We are writing code with our **future selves in mind**. It is important that humans can quickly understand the algorithm behind the code. Consider these two solutions:

```
# add up the number of creatures in a data file
reader = open('the-name.txt', 'r)
total = 0
for line in reader:
    if not(line.startswith('#') or line.startswith('Date')):
        temp = line.strip()
        fields = temp.split(',')
        count = int(fields[2])
        total = total + count
reader.close()
print('total number of creatures seen', total)
```

compared to this version


```
# add up the number of creatures in a data file
reader = open('the-name.txt', 'r)
total = 0
for line in reader:
    if (line.startswith('#') or line.startswith('Date')):
        pass # skip comments and data set title lines
    else:
        temp = line.strip()
        fields = temp.split(',')
        count = int(fields[2])
        total = total + count
reader.close()
print('total number of creatures seen', total)
```

The second is more quickly understood.



## Installing the software on Windows

Getting the software set up on Windows can be a challenge. I like the Msys2 system. 

Get the basic installer [here](https://msys2.github.io/). There are great packages on [github](https://github.com/Alexpux/MSYS2-packages), and they are easily installed using **`pacman`**. 

1. Download and install the appropriate [**msys2 installer**](https://msys2.github.io/). I like to install it in **`C:/Apps/msys64/`**.
2. Update the pacman database: `pacman -Sy pacman`
3. Install base devel: `pacman -S base-devel`
4. Install sqlite: `pacman -S sqlite`
5. Install nano: `pacman -S nano`
6. Install ruby: `pacman -S ruby` (optional dependency for svn)
7. Install svn: `pacman -S svn`

## An example of building up a workflow using a shell

We will analyze the `fish.txt` file. Use the `up-arrow` key to reduce typing

1. Remove the `Species` header

```
grep -v Species fish.txt
```

2. Extract just the species

```
up-arrow
grep -v Species fish.txt | cut -d , -f 2 
```

3. sort the species

```
up-arrow
grep -v Species fish.txt | cut -d , -f 2 | sort
```

4. Get just the species and count

```
up-arrow
grep -v Species fish.txt | cut -d , -f 2 | sort | uniq -c
```

Note the development process$\ldots$ The shell history is our friend$\ldots$
We have a command and trust it because we "grew it in stages".

**This is a habit we need to develop (in all our software development and problem solving)**. Think in terms of cycles of build/assemble then test$\ldots$ We need to 'check our work' repeatedly along the way.

You can tell a lot about whether a paper you are reviewing is likely to be correct by asking a few questions about the "how". Look at the build/test process. Is there a record (like saving the history$\ldots$)

For example, use:

```
history > second-record.txt
```

**There is no way to check** the 'how' inside a GUI tool!

Note we can search the history. Greg used

```
history | grep tail
```

to find this exemplar (at line 149):

```
cut -d , -f 2  fish.txt | sort | uniq -c | tail +2
```

which was the first way to count species. **Note:** we can re-run with 

```
!149
```

I tested with:

```
jrmFastMac [~/Documents/software-carpetry]
jrminter 41: wc tail-at-the-end.txt grep-at-the-start.txt 
       4       8      44 tail-at-the-end.txt
       4       8      44 grep-at-the-start.txt
       8      16      88 total

jrmFastMac [~/Documents/software-carpetry]
jrminter 42: diff tail-at-the-end.txt grep-at-the-start.txt 
```

## The Importance of using **predictable file names**


`fish.txt` is a **really bad name** for a data file! After a while one gets a series of files that have unhelpful names... 

Que PhD Comics cartoon$\ldots$

![](./png/phd101212s.png)

We need to use meaningful file-names and consistent data entry or data cleaning becomes a nightmare!

**The more consistent the data and the file name**, the easier it is to extract data


We can Use the `*` wildcard to get the shell to match for all characters!

Consistent file names help. Can use mv to rename and move files to consistent names and directories.

**This makes file selection and data cleaning easier!**


**THE KEY IDEAs: **

1. Use consistent file naming conventions
2. Use consistent file content/structure
3. Use the pattern matching from the shell and iPython terminal to reduce typing and errors.
4. **Test early and incrementally** to minimize errors. 
5. Encapsulation (the idea that objects contain data and methods, that is they contain things and know how to do things, is probably the most important idea when it comes to structuring large programs.
6. Use pipelines.
7. When changes are all localized, this is an indicator of good design. e.g. - we want to print in one place (function) so that format is consistent. Otherwise, sooner or later I will forget to change one... 



## Some key **`sqlite`** commands:

### Create the database by

```
sqlite3 experiments.db < create-db.sql
```

### Do a query

```
sqlite3 experiments.db < project-hours.sql
```




# Good advice from Neal Anderson

From [Practical Process Research and Development - A guide for Organic Chemists, Second Edition 2nd Edition, p. 19](http://www.amazon.com/dp/0123865379). Hat Tip [ChemJobber](http://chemjobber.blogspot.co.uk/2016/02/bonus-process-wednesday-life-tips-from.html).

> The attitudes for chemists in process R&D may differ from those of drug discovery chemists.

> 1. Teamwork is key. Optimize interactions between discovery chemists, engineers, analysts. microbiologists, operators, Environmental Health & Safety officer, pharmaceutical chemists. QA/QC personnel, attorneys for patent applications, out-sourcing (CROs, CMOs), your manufacturing counterparts, and regulatory personnel. **Don't trivialize the work of others, especially if you don't understand their job.** (emphasis mine)
2. Recognize that the need to minimize impurities drives process development.
3. Know the edge of failure for your operations. For rapid development, stay away from the edge. For cost-effective processes, you may want to operate close to the edge.
4. Reproducible, accurate analytical data are necessary for effective process development.
5. If an operation is extremely difficult, reconsider. Perhaps additional information will simplify the operation.
6. Heed the advice of ecologists: "We're all in this together." Humor helps defuse the gravity of most difficult situations.
7. During process introduction and scale-up boring situations can be good - that means there are no problems to address.



# William Shockley on what makes an effective researcher
[Brian McGill](http://dynamicecology.wordpress.com/2014/01/23/william-shockley-on-what-makes-a-person-write-a-lot-of-papers-and-the-superstar-researcher-system/) has a nice summary on Shockley's view of research. He used publication as a metric and noted the factors that were required:

1. ability to think of a good problem
2. ability to work on it
3. ability to recognize a worthwhile result
4. ability to make a decision as to when to stop and write up the results
5. ability to write adequately
6. ability to profit constructively from criticism
7. determination to submit the paper to a journal
8. persistence in making changes (if necessary as a result of journal action).

Shockley then posits, what if the odds of a person clearing hurdle ``i`` from the list of 8 above is $p_i$? Then the rate of publishing papers for this individual should be proportional to $p_{1}p_{2}p_{3}\dots p_{8}$. This gives the multiplication of random variables needed to explain the lognormal distribution of productivity (Shockley goes on to note that if one person is 50% above average in each of the 8 areas then they will be 2460% more productive than average at the total process).

Another conclusion is that if you are really bad at just one factor ($p_{i}$ close to zero for just one $i$), it sinks your overall productivity. This is innate in the multiplicative model. This is a solid rationale for working on one's weakest required skill before enhancing one's best requirement - or at least collaborating with those skills who complement yours.


[Back to Index](../README.html)